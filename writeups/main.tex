\documentclass[10pt]{article}      % use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                       % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                          % ... or a4paper or a5paper or ... 
%\geometry{landscape}                       % Activate for rotated page geometry
%\usepackage[parfill]{parskip}          % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}               % Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
                                % TeX will automatically convert eps --> pdf in pdflatex   
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{justification=raggedright,singlelinecheck=false}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{bm}
\usepackage{multicol}
\usepackage[switch]{lineno}

\usepackage{tikz}
\usetikzlibrary{fit,positioning}

\newcommand{\cov}{\mathrm{cov}}

\usepackage{setspace}

\begin{document}
\title{Learned Cumulant Generating Functions Allow Efficient Adaptation of Neural Network Statistics}
\author{Luke Rast}
\maketitle

% manuscript formatting
\doublespacing
\linenumbers


\section{Abstract}

Reformulate




Leveraging duality 




\section{Introduction and Background}



In order to make this paper tight, we must show that the learned CGF solves multiple issues related to adaptation / distribution change.
We already have
\begin{enumerate}
  \item Change point detection via rate function
  \item adaptation through training on tilted distribution
\end{enumerate}


\subsection{Intro the change detection problem}






\subsection{CGF}

The \textit{cumulant generating function} (CGF) of a random variable, $x$, is given by
\begin{equation}
  A(\theta) = \log \mathbb{E}_x[\exp(\theta^T x)]. \label{def:cgf}
\end{equation}
The CGF is the logarithm of the moment generating function.
When it exists, it provides a unique representation of a probability distribution.
Importantly, the cumulant generating function is guaranteed to be \textit{convex} [REF basic probability].


One application of the CGF is constructing families of \textit{exponentially tilted} probability distributions \cite{morris_natural_1982,morris_unifying_2009}.
Starting from a baseline distribution $h_0(x)$, with cumulant generating function $A_0(\theta)$, exponential tilting produces the family
\begin{equation}
  p_0(x | \theta) = h_0(x) e^{\theta^T x - A_0(\theta)}. \label{def:exponential_tilt}
\end{equation}
This is a natural exponential family of distributions, each of which modifies the base measure $h_0$ by concentrating the probability density along different directions to different extents.
The CGF also produces cumulant generating functions for the whole family: ${K_0(t | \theta) = A_0(\theta + t) - A_0(\theta)}$.
Thus the mean of any distribution in the family is given by the Jacobian of the CGF,
\begin{equation}
  \mu = \nabla_\theta A(\theta) \label{eq:cgf_jacobian}
\end{equation}
while the Hessian gives the covariance matrix, and so forth [REF basic probability].


Another application of the CGF concerns the theory of large deviations \cite{dembo2009large,touchette_basic_2012}, which describes the rate at which rare events become more and more unlikely as we collect more samples. 
For a series of probability measures, $p_n$, large deviations theory gives a \textit{rate function}, $I(x)$ that, loosely speaking, satisfies
\begin{equation}
  p_n(x \in \Gamma) \sim \exp \left( - n \min_{x \in \Gamma} I(x) \right) \quad \textrm{as} \quad n \to \infty \label{def:large_deviations}
\end{equation}
for any measurable set $\Gamma$.
That is, the probability of outcomes in the set $\Gamma$ shrinks exponentially with $n$ at a rate that is dominated by the smallest value of $I(x)$ on the set $\Gamma$ (see \cite{dembo2009large} for more precise expressions).
In particular, if we consider the distribution of the average $\mu_n = \frac{1}{n} \sum_{i=1}^n X_i$ of $n$ i.i.d. data points in $\mathbb{R}^d$, Cramer's theorem \cite{dembo2009large}, gives the corresponding rate function
\begin{equation}
  I(\mu) = \sup_{\theta}( \theta \mu - A(\theta) ). \label{eq:legendre_transform}
\end{equation}
The rate function is the Legendre transform of the cumulant generating function.
This rate function also gives rise to an asymptotically tight (as $n \to \infty$) probability density over means \cite{iltis_sharp_1995,chaganty_multidimensional_1986}
\begin{equation}
  p(\mu; n) \approx \frac{n}{2\pi}^{\frac{d}{2}} |V|^{-1/2} \exp(-n I(\mu)). \label{def:mean_density}
\end{equation}
Where $n$ is the number of samples, $d$ is the dimensionality of the data, and $V = {\bm H}[A](\theta(\mu))$ is covariance matrix given by the Hessian of the CGF.


To summarize these previous results, we have two related functions, $A(\theta) \leftrightarrow I(\mu)$, the cumulant generating function and rate function, 
related through Legendre duality.
Their arguments provide two related parameterizations $\theta \leftrightarrow \mu$ of the exponentially tilted family, corresponding to the natural parameter $\theta$ and the mean parameter, $\mu$.
Due to the duality, these are related by 
\begin{eqnarray}
  \mu = \nabla_\theta A(\theta) & \theta = \nabla_\mu I(\mu). \label{eq:duality_relations}
\end{eqnarray}
We apply these dual functions to the twin problems of change detection and adaptation.
On one side of this duality, the rate function $I(\mu)$, gives the probability density over the mean of many samples.
We use this density to detect changes in the statistics of the world, based on unlikely mean values.
On the other side of the duality, the CGF provides the basis for a family of modified versions of environmental statistics.
We use this family to adapt to changes in the statistics of the world. 

Here, we focus on change detection and adaptation in the context of neural networks. 
We start with trained neural networks, and aim to track and respond to changes in the statistic of one of the hidden layers. 
We do this by learning a second neural network, which is trained to reproduce the empirical cumulant generating function for the activity of this hidden layer.
As an input convex neural network \cite{amos_input_2017} this representation of the CGF is guaranteed to be convex and is easily differentiable, giving access to both the CGF and the rate function.
We take advantage of the rate function...





\subsection{Previous work}

Previous works on adaptation more generally

Previous works on exponential tilting: finance, ML, adaptation 


What is the connection to previous free energy based approaches?


Averaging enough data points will allow us to determine whether the network activity follows the same statistics as the training data using either element-wise test on each dimension, as in \cite{rabanser_failing_2019} or a joint test on the all dimensions simultaneously.


\subsubsection{Connection to VAEs}
This is where the exponential family analogy belongs.


Connection to VAEs \cite{kingma_auto-encoding_2013}: inner vs outer relaxation of inference.
VAEs learn to perform approximate inference with specified noise and latent variable distributions, while the approach here performs exact inference using learned noise distributions and their conjugates. 




Learning an exponential family by learning sufficient statistic $T(x)$ and the base measure $h(x)$, here we use the features learned by a pre-trained neural network as sufficient statistics, and capture their distribution with learned moment generating function $A(\theta)$.


First, the exponentially tilted distributions become reconizably exponential families
\begin{equation}
  p(T | \theta) = h(T) \exp( \theta^T T - A(\theta) ),
\end{equation}



The parameter $\mu$ is the derivative of the CGF, and therefore is equal to the average value of $T$ for data generated from the corresponding distribution $p(T|\theta)$.
Thus, $I(\mu)$ is a function of observation \textit{averages}.

and the same process can be extended to produce any exponential family, by combining a set of statistics of the data, $T(x)$, and a base distribution of these statistics.
This is the path that we pursue in this paper: we will learn exponential families by learning both $T(x)$ and $M(\theta)$ and use these learned families to perform inference.





\section{Results}


The empirical cumulant generating function uses the empirical mean to evaluate the expectation in the CGF
\begin{equation}
  \hat A(\theta) = \log \sum_{x_i} \exp(\theta x).
\end{equation}
This expression has been used as a estimator for the CGF \cite{duffield_entropy_1995} [others ???]. 
Given a trained neural neural network, we choose one hidden layer of the network to be our feature vector.
We train an ICNN to capture the distribution of features on the training set by learning the empirical CGF for these features.
That is to say, given input output pairs $\{ (\theta, \hat A(\theta) ) \}$, we train our input convex network to reproduce $\hat A(\theta)$.
While we have a limited number of datapoints $x$ with which to evaluate the empirical CGF, we can create as many $\theta$ training points as required to learn the empirical CGF to desired precision.

Question: how many training samples do I need as a function of dimensionality to learn the CGF? This could be an argument for using the dimensionality reduction by the neural network features.

Other things to write about: features of the eCGF, limiting slopes; feature of the learned versions.



\subsection{Change point detection}
The learned CGF describes the feature statistics on the training set.
We use the corresponding rate function to detect changes in these statistics that appear in the many sample feature averages. 


Given the average value of our features on run-time data, we use the limiting probability density eq.~(\ref{def:mean_density}) to determine whether such an average represents a significant deviation from the training set.

We can view this as a hypothesis test

Find sets with a certain p-value, corresponding to the power? of our test

Use level curves of the rate function as our family of sets

Use importance sampling to avoid the curse of dimensionality to find probability of falling within a certain set.

binary search to find the threashold that corresponds to our target p-value.




Steps:
\begin{enumerate}
  \item Show sensitivity (and lack of sensitivity)
\end{enumerate}



\subsection{Exponential tilting}

When a significant deviation from the training set is detected, we then wish to correct for this deviation in our model.

We can accomplish these corrections by modelling the new feature distribution as an exponentially tilted (see eq.~(\ref{def:exponential_tilt})) 

One way to do this is by fine-tuning the top half of the network on the training set with loss function weights, as given by [REF]



Steps / questions:
\begin{enumerate}
  \item retraining
  \item class conditional distribution
  \item Use for Bayesian inference?
\end{enumerate}



\subsection{Adaptation Through Fine tuning}





\subsection{Target conditional CGF}





\subsection{Adaptation Through Activity Remapping}






\subsection{Measurement of Fisher Information}

Establish the method for measuring the Fisher information of data. In particular, with respect to changes in a known parameter, which can be trained.




\section{Discussion}

Future direction: learning $T(x)$ together with sufficient statistics.


and the same process can be extended to produce any exponential family, by combining a set of statistics of the data, $T(x)$, and a base distribution of these statistics.
This is the path that we pursue in this paper: we will learn exponential families by learning both $T(x)$ and $M(\theta)$ and use these learned families to perform inference.



\section{Methods}
\subsection{Network architecture}

We fit the empirical cumulant generating functions using input convex neural networks \cite{amos_input_2017}, which are guaranteed to learn convex functions.
We use a multi-layer perceptron with no skip connections, but with positive weight matrices initialized as in \cite{hoedt_principled_2023}, and guaranteed to be positive by a ReLU applied to the weights.
For non-linearities, we use a leaky version of a softplus, given by $sp_{\textrm{leaky}}(x) = \alpha x + (1 - \alpha) sp(x) + c_0$ where $sp(x)$ is a softplus function, $\alpha$ sets the leak slope for negative values, and $c_0$ sets the intercept. 
These values are set by hyper-parameter sweep (???). 
The leaky softplus non-linearity has appealing properties:
It is continuously differentiable to at least second order, giving smooth Jacobians and Hessians.
It retains benefits of leaky ReLUs in preventing dead units.
Finally, it allows negative values to pass through the network, which is particularly important in the ICNN case, where the network weights are all positive.




\subsection{Duality Computations}
Given a neural network representation of the cumulant generating function $A(\theta)$, we want to be able to transform between parameter and moment representations $\theta \leftrightarrow \mu$, and to evaluate both CGF and rate function values, $A(\theta)$ and $I(\mu)$.
Using the trained network, $A(\theta)$ is directly computed by evaluation, while the parameter to moment mapping $\mu(\theta)$ is given by its Jacobian (eq.~\ref{eq:duality_relations}).
Evaluation of $I(\mu)$ and $\mu(\theta)$ both come down to evaluating the Legendre transform (eq.~\ref{eq:legendre_transform}). 
We directly solve the optimization problem by gradient descent in $\theta$, with the resulting optimal input giving $\theta(\mu)$, while the optimal value is $I(\mu)$.
This works well enough for our purposes, but more sophisticated optimization and differential equations based approaches are available as well [REF???].



\section{OLD MATERIAL}

The Fisher information matrix for this family is given by the Hessian of $A(\theta)$ 
\begin{equation}
  I(\theta) = \bm{H}_\theta A(\theta) 
\end{equation}


\bibliography{main.bib}
\bibliographystyle{ieeetr}

\end{document}